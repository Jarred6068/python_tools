{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 1 source code:\n",
    "\n",
    "#I used OOP to streamline changing between a number of model structures. The first block below is code to\n",
    "#define the Mymodel class and functions\n",
    "\n",
    "#see the Mymodel.preprocess function for details on how the data was processed for analysis\n",
    "\n",
    "#the second block contains the actual commands to run and execute the model construction and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras as K\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "\n",
    "def X_convert_onehot(X):\n",
    "    \n",
    "    for i in np.arange(X.shape[1]):\n",
    "    \n",
    "        if i == 0:\n",
    "            #set new x matrix for onehot\n",
    "            X_onehot = K.utils.to_categorical(X[:,i]-1)    \n",
    "        else:\n",
    "            #onehot encode\n",
    "            onehot = K.utils.to_categorical(X[:,i]-1)\n",
    "            #append onehot columns to matrix\n",
    "            X_onehot = np.append(X_onehot, onehot, axis = 1)\n",
    "            \n",
    "    return X_onehot\n",
    "\n",
    "\n",
    "\n",
    "class Mymodel():\n",
    "    \n",
    "    \"\"\"\n",
    "    A Keras model constructed on the COVID 19 data found here:\n",
    "    https://www.kaggle.com/datasets/meirnizri/covid19-dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, objective = ['pred_outcome', 'pred_risk'],\n",
    "                 pathname = ''):\n",
    "        \"\"\"\n",
    "        Initialization of keras model constructed on Covid 19 model to assess risk/outcome\n",
    "        \n",
    "        \n",
    "        objective: either pred_outcome or pred_risk. Outcome is a binary prediction problem to predict\n",
    "                   whether a patient died or survived their covid infection. Risk is an 8-class prediction\n",
    "                   problem which assess an individuals health risk by combing their clinical attributes\n",
    "                   for ICU, INTUBED, and CLINICAL OUTCOME\n",
    "                   \n",
    "        pathname = /path/to/the/file/Covid Data.csv\n",
    "                   \n",
    "                  \n",
    "        \n",
    "        \"\"\"\n",
    "        #read in data\n",
    "        self.covid_data = pd.read_csv(pathname+'/Covid Data.csv')\n",
    "        #set the objective\n",
    "        self.objective = objective\n",
    "    \n",
    "    def preprocess(self, convert_X = ['Onehot','Sum_coding'],\n",
    "                   drop_features_with_high_nan = False):\n",
    "        \"\"\"\n",
    "        This function automatically performs the preprocessing steps needed to get the covid data\n",
    "        read for analysis\n",
    "        \n",
    "        convert_X: how to conver the categorical features (only works for Onehot right now)\n",
    "        \n",
    "        drop_features_with_high_nan: whether to remove features with more than 80% NaN values (ICU and INTUBED)\n",
    "\n",
    "        \"\"\"\n",
    "        #replace 'missing' pregnancy values for males (97 or 98) as 2 ('no')\n",
    "        self.covid_data.loc[self.covid_data.SEX == 2, 'PREGNANT'] = 2\n",
    "\n",
    "\n",
    "\n",
    "        #replace all 97,98, and 99 values with NaNs\n",
    "        covid_data_new = self.covid_data.replace([97,98,99], np.nan)\n",
    "\n",
    "\n",
    "        #now we are going to add a new column called clinical outcome \n",
    "        covid_data_new = covid_data_new.assign(CLINICAL_OUTCOME = self.covid_data.DATE_DIED)\n",
    "        covid_data_new.drop(columns = 'DATE_DIED', inplace = True)\n",
    "        #we will use the 9999-99-99 which indicate the patient did not die as a proxy for 'lived' and\n",
    "        #any other date as a proxy for 'died'\n",
    "        #we will set 1 as died and 2 as lived\n",
    "        covid_data_new.CLINICAL_OUTCOME.replace('9999-99-99', 2, inplace = True)\n",
    "        which = np.unique(covid_data_new.CLINICAL_OUTCOME[covid_data_new.CLINICAL_OUTCOME != 2])\n",
    "        covid_data_new.CLINICAL_OUTCOME.replace(which.tolist(), 1, inplace = True)\n",
    "        \n",
    "        \n",
    "        #option to drop features with a high percentage of nan\n",
    "        if (drop_features_with_high_nan == True) & (self.objective == 'pred_outcome'):\n",
    "            \n",
    "            for col in covid_data_new.columns:\n",
    "                \n",
    "                z = sum(np.isnan(covid_data_new[col]))/len(covid_data_new[col])\n",
    "                print('proportion of NaNs for feature {}, is {}'.format(\n",
    "                    col, z))\n",
    "                \n",
    "                if sum(np.isnan(covid_data_new[col]))/len(covid_data_new[col]) > 0.8:\n",
    "                    #if number of nan's over 80% drop feature\n",
    "                    covid_data_new.drop(columns = col, inplace = True)\n",
    "                \n",
    "            #remove rows with nans    \n",
    "            covdata_nona = covid_data_new.dropna(axis = 0)\n",
    "            print('---- size of data after column drop and nan removal: rows,cols {}, -----'.format(\n",
    "            covdata_nona.shape))\n",
    "                \n",
    "        else:\n",
    "            #remove rows with nans\n",
    "            covdata_nona = covid_data_new.dropna(axis = 0)    \n",
    "            print('---- size of data after column drop and nan removal: rows,cols {}, -----'.format(\n",
    "                  covdata_nona.shape))\n",
    "            \n",
    "        #discretize age into age groups by rounding\n",
    "        covdata_nona = covdata_nona.assign(AGE = np.round(covdata_nona.AGE, -1))\n",
    "    \n",
    "\n",
    "        #keep only samples with values between 1-3. values over 4 means covid test negative/inconclusive\n",
    "        covdata_nona_final = covdata_nona[covdata_nona.CLASIFFICATION_FINAL < 4]\n",
    "        if (drop_features_with_high_nan != True) & (self.objective == 'pred_risk'):\n",
    "            #discretize age into age groups by rounding\n",
    "            covdata_nona = covdata_nona.assign(AGE = np.round(covdata_nona.AGE, -1))\n",
    "    \n",
    "\n",
    "            #keep only samples with values between 1-3. values over 4 means covid test negative/inconclusive\n",
    "            covdata_nona_final = covdata_nona[covdata_nona.CLASIFFICATION_FINAL < 4]\n",
    "            #final dimension (108090, 22)\n",
    "\n",
    "\n",
    "            #-------Createdting--Clinical--Risk--proxy--target--variable---------\n",
    "            #our clinical risk is a combination of outcome (lived/died), intubed (intubated/not), and\n",
    "            # icu (admitted/not)\n",
    "            #convert CLINICAL_OUTCOME, ICU and INTUBED to unique strings\n",
    "            outcomes_as_strings = covdata_nona_final.ICU.replace([1,2], ['Died_','Recovered_'])\n",
    "            icu_as_strings = covdata_nona_final.ICU.replace([1,2], ['Admitted_ICU_','Not_Admitted_ICU_'])\n",
    "            intubed_as_strings = covdata_nona_final.INTUBED.replace([1,2], ['Intubated', 'Not_Intubated'])\n",
    "\n",
    "            #combine the strings element wise from Outcome, ICU, and Intubed\n",
    "            l1 = [i + j for i, j in zip(outcomes_as_strings, icu_as_strings)]\n",
    "            l2 = [i + j for i, j in zip(l1, intubed_as_strings)]\n",
    "            #set new variable clinical risk as the element-wise joined list l2\n",
    "            covdata_nona_final = covdata_nona_final.assign(CLINICAL_RISK = l2)\n",
    "            #now we need to recode the unique strings back into numerical strings\n",
    "            covdata_nona_final = covdata_nona_final.assign(CLINICAL_RISK_FINAL = covdata_nona_final.CLINICAL_RISK)\n",
    "            #obtain the number of risk classes\n",
    "            classes = np.unique(covdata_nona_final['CLINICAL_RISK_FINAL']).tolist()\n",
    "            #recode to 0-7\n",
    "            covdata_nona_final.CLINICAL_RISK_FINAL.replace(classes, np.arange(len(classes)).tolist(), inplace = True)\n",
    "\n",
    "\n",
    "        #Set Y and X for model            \n",
    "        if self.objective == 'pred_risk':\n",
    "                \n",
    "            self.Y = np.array(covdata_nona_final.CLINICAL_RISK_FINAL)\n",
    "            #Drop Columns we created or used to create clinical risk\n",
    "            self.covid_data_final = covdata_nona_final.drop(columns=['CLINICAL_OUTCOME', 'CLINICAL_RISK', 'CLINICAL_RISK_FINAL'])\n",
    "            self.X = np.array(self.covid_data_final)\n",
    "                \n",
    "        if self.objective == 'pred_outcome':\n",
    "            self.Y = np.array(covdata_nona_final.CLINICAL_OUTCOME)\n",
    "            if drop_features_with_high_nan == True:\n",
    "                self.covid_data_final = covdata_nona_final.drop(columns=['CLINICAL_OUTCOME'])\n",
    "            else: \n",
    "                self.covid_data_final = covdata_nona_final.drop(columns=['CLINICAL_RISK', 'CLINICAL_RISK_FINAL', 'CLINICAL_OUTCOME'])\n",
    "                \n",
    "            self.X = np.array(self.covid_data_final)\n",
    "        \n",
    "        \n",
    "        print('final size of feature matrix: rows,cols {}, with features ={}'.format(\n",
    "                  self.covid_data_final.shape, self.covid_data_final.columns))\n",
    "        #convert X into a one-hot matrix for all categorical variables\n",
    "        if convert_X == 'Onehot':\n",
    "            self.X_convert = X_convert_onehot(self.X)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def build_model(self, conv_layers = None, mlp_layers = 3, mlp_hidden_act = ['leakyrelu'],\n",
    "                    hidden_units = [50], conv_hidden_act= ['relu'],  padd_type = 'same', \n",
    "                    num_filters = [32], kernels = [3], layers_stride = 2, add_norm = False, \n",
    "                    pool_type = ['Avg', 'Max'], use_metric = 'accuracy', use_batch_norm = True, \n",
    "                    use_dropout_layers = True, dropout_rate = 0.2, use_encoding_layer = True,\n",
    "                    out_mode = ['one_hot','multi_hot','count'], verbose = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function wraps keras.model.sequential to build a keras model consisting of convolution and\n",
    "        Dense Layers\n",
    "        \n",
    "        \"\"\"\n",
    "        self.encoding_layer = use_encoding_layer\n",
    "        self.conv_layers = conv_layers\n",
    "        self.mlp_layers = mlp_layers\n",
    "        self.model = K.models.Sequential()\n",
    "        \n",
    "        if add_norm == True:\n",
    "            self.model.add(layers.Normalization(axis=1))\n",
    "            \n",
    "        if use_encoding_layer == True:\n",
    "            self.model.add(layers.CategoryEncoding(output_mode = out_mode))\n",
    "        \n",
    "        \n",
    "        #convolutional layers\n",
    "        if conv_layers != None:\n",
    "            \n",
    "            #set input shape \n",
    "            in_shape_conv = (self.X_convert.shape[0], self.X_convert.shape[1], 1)\n",
    "        \n",
    "            for i in np.arange(conv_layers):\n",
    "                \n",
    "                if i == 0:     \n",
    "                    #conv 2D layer\n",
    "                    self.model.add(layers.Conv1D(filters = num_filters[i], \n",
    "                                                 kernel_size = kernels[i],\n",
    "                                                 padding = padd_type,\n",
    "                                                 input_shape = in_shape_conv[1:],\n",
    "                                                 data_format=\"channels_last\",\n",
    "                                                 strides = layers_stride))\n",
    "                    #if batch normalization\n",
    "                    if use_batch_norm == True:\n",
    "                        self.model.add(layers.BatchNormalization())\n",
    "                    \n",
    "                    \n",
    "                    #activate\n",
    "                    self.model.add(layers.Activation(conv_hidden_act[i]))\n",
    "                        \n",
    "                    #pooling layer\n",
    "                    if pool_type == 'Max':\n",
    "                        self.model.add(layers.GlobalMaxPooling1D())\n",
    "                    \n",
    "                    if pool_type == 'Avg':\n",
    "                        self.model.add(layers.GlobalAveragePooling1D())\n",
    "                        \n",
    "                else:\n",
    "                    #conv 2D layer\n",
    "                    self.model.add(layers.Conv1D(filters = num_filters[i], \n",
    "                                                 kernel_size = kernels[i],\n",
    "                                                 padding = padd_type,\n",
    "                                                 input_shape = in_shape_conv[1:],\n",
    "                                                 data_format=\"channels_last\",\n",
    "                                                 strides = layers_stride))\n",
    "                    #if batch normalization\n",
    "                    if use_batch_norm == True:\n",
    "                        self.model.add(layers.BatchNormalization())\n",
    "                    \n",
    "                    \n",
    "                    #activate\n",
    "                    self.model.add(layers.Activation(conv_hidden_act[i]))\n",
    "                        \n",
    "                    #pooling layer\n",
    "                    if pool_type == 'Max':\n",
    "                        self.model.add(layers.GlobalMaxPooling1D())\n",
    "                    \n",
    "                    if pool_type == 'Avg':\n",
    "                        self.model.add(layers.GlobalAveragePooling1D())\n",
    "                        \n",
    "                    \n",
    "            #flattening layer\n",
    "            self.model.add(layers.Flatten())\n",
    "        \n",
    "        #input dimension for dense layers (only when no convolution layers are used)\n",
    "        in_shape_dense = self.X_convert.shape\n",
    "        \n",
    "        #perceptron (dense) layers\n",
    "        for i in np.arange(mlp_layers):\n",
    "            \n",
    "            #first hidden layer\n",
    "            if i == 0:\n",
    "                if conv_layers != None:\n",
    "                    #if convolution layers no need to set input dim\n",
    "                    #dense layer\n",
    "                    self.model.add(layers.Dense(units = hidden_units[i],\n",
    "                                                name = 'hidden_layer'+str(i)))\n",
    "                    #if batch normalization\n",
    "                    if use_batch_norm == True:\n",
    "                        self.model.add(layers.BatchNormalization())\n",
    "                    \n",
    "                    #activate\n",
    "                    self.model.add(layers.Activation(mlp_hidden_act[i]))\n",
    "                    \n",
    "                    #dropout layer\n",
    "                    if use_dropout_layers == True:\n",
    "                        self.model.add(layers.Dropout(dropout_rate))\n",
    "                \n",
    "                else:\n",
    "                    #if no conv layers need to set input dimension to # of features\n",
    "                    self.model.add(layers.Dense(units = hidden_units[i], \n",
    "                                                input_dim = in_shape_dense[1],\n",
    "                                                name = 'hidden_layer'+str(i)))\n",
    "                    \n",
    "                    #if batch normalization\n",
    "                    if use_batch_norm == True:\n",
    "                        self.model.add(layers.BatchNormalization())\n",
    "                    \n",
    "                    #activate\n",
    "                    self.model.add(layers.Activation(mlp_hidden_act[i]))\n",
    "                    \n",
    "                    \n",
    "                    #dropout layer\n",
    "                    if use_dropout_layers == True:\n",
    "                        self.model.add(layers.Dropout(dropout_rate))\n",
    "            \n",
    "            #all other hidden layers\n",
    "            else:\n",
    "                #\n",
    "                self.model.add(layers.Dense(units = hidden_units[i],\n",
    "                                            name = 'hidden_layer'+str(i)))\n",
    "                \n",
    "                if use_batch_norm == True:\n",
    "                        self.model.add(layers.BatchNormalization())\n",
    "                        \n",
    "                self.model.add(layers.Activation(mlp_hidden_act[i]))\n",
    "                \n",
    "                #dropout layer\n",
    "                if use_dropout_layers == True:\n",
    "                    self.model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        \n",
    "        #Add output layer and compile model\n",
    "        #set optimizer to adam\n",
    "        opt = K.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        #for binary lived/died prediction (clinical outcome)\n",
    "        if self.objective == 'pred_outcome':\n",
    "            #Dense output layer\n",
    "            self.model.add(layers.Dense(units = 1, name = 'output_layer'))\n",
    "            #if batch normalization\n",
    "            if use_batch_norm == True:\n",
    "                self.model.add(layers.BatchNormalization())\n",
    "            \n",
    "            #activate\n",
    "            self.model.add(layers.Activation('sigmoid'))\n",
    "            \n",
    "            #compile model\n",
    "            loss_func = K.losses.BinaryCrossentropy(from_logits=False)\n",
    "            self.model.compile(loss=loss_func, optimizer=opt, metrics = 'accuracy') \n",
    "        \n",
    "        #for multiclass 'risk' prediction -- combined lived/died+ICU+Intubed\n",
    "        if self.objective == 'pred_risk':\n",
    "            #dense layer\n",
    "            self.model.add(layers.Dense(units = len(np.unique(self.Y)),\n",
    "                                            name = 'output_layer'))\n",
    "            \n",
    "            \n",
    "            #if batch normalization\n",
    "            if use_batch_norm == True:\n",
    "                self.model.add(layers.BatchNormalization())\n",
    "            \n",
    "            #activate\n",
    "            self.model.add(layers.Activation('softmax'))\n",
    "            \n",
    "            \n",
    "            #compile model\n",
    "            loss_func = K.losses.CategoricalCrossentropy()\n",
    "            self.model.compile(loss=loss_func, optimizer=opt, metrics = 'accuracy')\n",
    "        \n",
    "        #print final model\n",
    "        if verbose == True:\n",
    "            print(self.model.summary())\n",
    "      \n",
    "            \n",
    "      \n",
    "        \n",
    "    def fit(self, train_size = 0.9, validation_size = 0.2, batchsize = 128,\n",
    "            num_epochs = 50):\n",
    "        \n",
    "        if self.encoding_layer == True:\n",
    "            \n",
    "            self.x_train, self.x_test, self.y_train, self.y_test = tts(self.X, \n",
    "                                                                       self.Y, \n",
    "                                                                       test_size = 1 - train_size, \n",
    "                                                                       train_size = train_size)\n",
    "            \n",
    "        else:\n",
    "            self.x_train, self.x_test, self.y_train, self.y_test = tts(self.X_convert, \n",
    "                                                                       self.Y, \n",
    "                                                                       test_size = 1 - train_size, \n",
    "                                                                       train_size = train_size)\n",
    "        \n",
    "        if self.conv_layers != None:\n",
    "            train_shp = self.x_train.shape\n",
    "            test_shp = self.x_test.shape\n",
    "            self.x_train = self.x_train.reshape((train_shp[0], train_shp[1], 1))\n",
    "            self.x_test = self.x_test.reshape((test_shp[0], test_shp[1], 1))\n",
    "            \n",
    "            \n",
    "        if self.objective == 'pred_outcome':\n",
    "            self.y_train_convert = self.y_train-1\n",
    "            self.y_test_convert = self.y_test-1\n",
    "            \n",
    "        if self.objective == 'pred_risk':\n",
    "            self.y_train_convert = K.utils.to_categorical(self.y_train)       \n",
    "            self.y_test_convert = K.utils.to_categorical(self.y_test) \n",
    "            \n",
    "        \n",
    "            \n",
    "        self.model.fit(self.x_train, \n",
    "                       self.y_train_convert, \n",
    "                       batch_size = batchsize, \n",
    "                       epochs = num_epochs, \n",
    "                       validation_split=validation_size, \n",
    "                       validation_batch_size= round(validation_size*batchsize))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate(self, batchsize = 256):\n",
    "        \n",
    "        self.model.evaluate(x = self.x_test,\n",
    "                            y = self.y_test_convert,\n",
    "                            batch_size=batchsize)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def model_arch(hp, numlayers = 5, \n",
    "               units_in_layers = [[150,120,100], [75, 60, 50], [35, 30, 25], [20, 15, 10], [10, 8, 5]],\n",
    "               use_batch_norm = True, use_dropout_layers = True, dropout_rate = 0.2):\n",
    "        \n",
    "    model = K.models.Sequential()\n",
    "    for i in np.arange(numlayers):\n",
    "        model.add(layers.Dense(hp.Choice('units',units_in_layers[i])))\n",
    "        if use_batch_norm == True:\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        model.add(layers.Activation('relu'))\n",
    "            \n",
    "        if use_dropout_layers == True:\n",
    "            model.add(layers.Dropout(dropout_rate))\n",
    "            \n",
    "    model.add(layers.Dense(units = 1))\n",
    "    if use_batch_norm == True:\n",
    "        model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "        \n",
    "    if use_dropout_layers == True:\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    opt = K.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_func = K.losses.BinaryCrossentropy(from_logits=False)\n",
    "    model.compile(loss=loss_func, optimizer=opt, metrics = 'accuracy') \n",
    "    \n",
    "    #print(model.summary())\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "def tune_arch(x, x_val, y, y_val):\n",
    "            \n",
    "    arch_tuner = keras_tuner.RandomSearch(\n",
    "        model_arch,\n",
    "        objective='val_loss',\n",
    "        max_trials=5)\n",
    "    \n",
    "    arch_tuner.search(x, y, epochs=5, validation_data=(x_val, y_val))\n",
    "    best_model = arch_tuner.get_best_models()[0]\n",
    "    \n",
    "    return best_model\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 04m 46s]\n",
      "val_loss: 0.34661296010017395\n",
      "\n",
      "Best val_loss So Far: 0.3340647220611572\n",
      "Total elapsed time: 00h 12m 24s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer0 (Dense)       (None, 120)               20040     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 120)              480       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 120)               0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " hidden_layer1 (Dense)       (None, 60)                7260      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 60)               240       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 60)                0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 60)                0         \n",
      "                                                                 \n",
      " hidden_layer2 (Dense)       (None, 30)                1830      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 30)               120       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 30)                0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " hidden_layer3 (Dense)       (None, 20)                620       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " hidden_layer4 (Dense)       (None, 10)                210       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 11        \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 1)                4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,935\n",
      "Trainable params: 30,453\n",
      "Non-trainable params: 482\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "2180/2180 [==============================] - 23s 8ms/step - loss: 0.4032 - accuracy: 0.8507 - val_loss: 0.2633 - val_accuracy: 0.8955\n",
      "Epoch 2/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2530 - accuracy: 0.8860 - val_loss: 0.2218 - val_accuracy: 0.8965\n",
      "Epoch 3/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2285 - accuracy: 0.8863 - val_loss: 0.2112 - val_accuracy: 0.8954\n",
      "Epoch 4/50\n",
      "2180/2180 [==============================] - 21s 10ms/step - loss: 0.2218 - accuracy: 0.8885 - val_loss: 0.2073 - val_accuracy: 0.8960\n",
      "Epoch 5/50\n",
      "2180/2180 [==============================] - 25s 12ms/step - loss: 0.2194 - accuracy: 0.8874 - val_loss: 0.2071 - val_accuracy: 0.8961\n",
      "Epoch 6/50\n",
      "2180/2180 [==============================] - 24s 11ms/step - loss: 0.2182 - accuracy: 0.8881 - val_loss: 0.2055 - val_accuracy: 0.8967\n",
      "Epoch 7/50\n",
      "2180/2180 [==============================] - 20s 9ms/step - loss: 0.2167 - accuracy: 0.8892 - val_loss: 0.2052 - val_accuracy: 0.8968\n",
      "Epoch 8/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2164 - accuracy: 0.8888 - val_loss: 0.2053 - val_accuracy: 0.8963\n",
      "Epoch 9/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2159 - accuracy: 0.8892 - val_loss: 0.2045 - val_accuracy: 0.8968\n",
      "Epoch 10/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2155 - accuracy: 0.8898 - val_loss: 0.2044 - val_accuracy: 0.8968\n",
      "Epoch 11/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2153 - accuracy: 0.8895 - val_loss: 0.2050 - val_accuracy: 0.8967\n",
      "Epoch 12/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2154 - accuracy: 0.8894 - val_loss: 0.2037 - val_accuracy: 0.8974\n",
      "Epoch 13/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2151 - accuracy: 0.8900 - val_loss: 0.2049 - val_accuracy: 0.8962\n",
      "Epoch 14/50\n",
      "2180/2180 [==============================] - 19s 9ms/step - loss: 0.2151 - accuracy: 0.8896 - val_loss: 0.2052 - val_accuracy: 0.8968\n",
      "Epoch 15/50\n",
      "2180/2180 [==============================] - 20s 9ms/step - loss: 0.2149 - accuracy: 0.8901 - val_loss: 0.2036 - val_accuracy: 0.8972\n",
      "Epoch 16/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2145 - accuracy: 0.8902 - val_loss: 0.2037 - val_accuracy: 0.8973\n",
      "Epoch 17/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2144 - accuracy: 0.8897 - val_loss: 0.2039 - val_accuracy: 0.8970\n",
      "Epoch 18/50\n",
      "2180/2180 [==============================] - 16s 7ms/step - loss: 0.2142 - accuracy: 0.8895 - val_loss: 0.2052 - val_accuracy: 0.8954\n",
      "Epoch 19/50\n",
      "2180/2180 [==============================] - 16s 7ms/step - loss: 0.2140 - accuracy: 0.8895 - val_loss: 0.2038 - val_accuracy: 0.8973\n",
      "Epoch 20/50\n",
      "2180/2180 [==============================] - 16s 7ms/step - loss: 0.2142 - accuracy: 0.8902 - val_loss: 0.2040 - val_accuracy: 0.8970\n",
      "Epoch 21/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2138 - accuracy: 0.8899 - val_loss: 0.2043 - val_accuracy: 0.8971\n",
      "Epoch 22/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2135 - accuracy: 0.8907 - val_loss: 0.2042 - val_accuracy: 0.8965\n",
      "Epoch 23/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2130 - accuracy: 0.8917 - val_loss: 0.2040 - val_accuracy: 0.8973\n",
      "Epoch 24/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2131 - accuracy: 0.8917 - val_loss: 0.2041 - val_accuracy: 0.8971\n",
      "Epoch 25/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2129 - accuracy: 0.8910 - val_loss: 0.2038 - val_accuracy: 0.8970\n",
      "Epoch 26/50\n",
      "2180/2180 [==============================] - 17s 8ms/step - loss: 0.2133 - accuracy: 0.8912 - val_loss: 0.2038 - val_accuracy: 0.8969\n",
      "Epoch 27/50\n",
      "2180/2180 [==============================] - 14s 7ms/step - loss: 0.2123 - accuracy: 0.8914 - val_loss: 0.2035 - val_accuracy: 0.8971\n",
      "Epoch 28/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2129 - accuracy: 0.8919 - val_loss: 0.2038 - val_accuracy: 0.8970\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2180/2180 [==============================] - 16s 8ms/step - loss: 0.2132 - accuracy: 0.8912 - val_loss: 0.2045 - val_accuracy: 0.8966\n",
      "Epoch 30/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2127 - accuracy: 0.8913 - val_loss: 0.2038 - val_accuracy: 0.8967\n",
      "Epoch 31/50\n",
      "2180/2180 [==============================] - 13s 6ms/step - loss: 0.2125 - accuracy: 0.8917 - val_loss: 0.2038 - val_accuracy: 0.8969\n",
      "Epoch 32/50\n",
      "2180/2180 [==============================] - 13s 6ms/step - loss: 0.2123 - accuracy: 0.8917 - val_loss: 0.2039 - val_accuracy: 0.8971\n",
      "Epoch 33/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2122 - accuracy: 0.8921 - val_loss: 0.2041 - val_accuracy: 0.8969\n",
      "Epoch 34/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2122 - accuracy: 0.8919 - val_loss: 0.2039 - val_accuracy: 0.8973\n",
      "Epoch 35/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2131 - accuracy: 0.8909 - val_loss: 0.2046 - val_accuracy: 0.8972\n",
      "Epoch 36/50\n",
      "2180/2180 [==============================] - 16s 7ms/step - loss: 0.2123 - accuracy: 0.8922 - val_loss: 0.2039 - val_accuracy: 0.8973\n",
      "Epoch 37/50\n",
      "2180/2180 [==============================] - 13s 6ms/step - loss: 0.2123 - accuracy: 0.8917 - val_loss: 0.2037 - val_accuracy: 0.8974\n",
      "Epoch 38/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2117 - accuracy: 0.8924 - val_loss: 0.2034 - val_accuracy: 0.8976\n",
      "Epoch 39/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2127 - accuracy: 0.8914 - val_loss: 0.2037 - val_accuracy: 0.8969\n",
      "Epoch 40/50\n",
      "2180/2180 [==============================] - 13s 6ms/step - loss: 0.2120 - accuracy: 0.8918 - val_loss: 0.2037 - val_accuracy: 0.8973\n",
      "Epoch 41/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2120 - accuracy: 0.8921 - val_loss: 0.2040 - val_accuracy: 0.8971\n",
      "Epoch 42/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2115 - accuracy: 0.8923 - val_loss: 0.2036 - val_accuracy: 0.8968\n",
      "Epoch 43/50\n",
      "2180/2180 [==============================] - 14s 7ms/step - loss: 0.2118 - accuracy: 0.8921 - val_loss: 0.2043 - val_accuracy: 0.8973\n",
      "Epoch 44/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2121 - accuracy: 0.8921 - val_loss: 0.2038 - val_accuracy: 0.8976\n",
      "Epoch 45/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2117 - accuracy: 0.8920 - val_loss: 0.2050 - val_accuracy: 0.8962\n",
      "Epoch 46/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2114 - accuracy: 0.8926 - val_loss: 0.2036 - val_accuracy: 0.8969\n",
      "Epoch 47/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2118 - accuracy: 0.8929 - val_loss: 0.2041 - val_accuracy: 0.8972\n",
      "Epoch 48/50\n",
      "2180/2180 [==============================] - 18s 8ms/step - loss: 0.2114 - accuracy: 0.8918 - val_loss: 0.2037 - val_accuracy: 0.8970\n",
      "Epoch 49/50\n",
      "2180/2180 [==============================] - 15s 7ms/step - loss: 0.2114 - accuracy: 0.8924 - val_loss: 0.2040 - val_accuracy: 0.8972\n",
      "Epoch 50/50\n",
      "2180/2180 [==============================] - 14s 6ms/step - loss: 0.2115 - accuracy: 0.8920 - val_loss: 0.2041 - val_accuracy: 0.8973\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import sys\n",
    "#path = '/mnt/ceph/jarredk/Assignments/Deep_Learning/DLproject/'\n",
    "#sys.path.append(path)\n",
    "\n",
    "#path to covid data file\n",
    "path = r'C:\\Users\\Bruin\\Desktop\\GS Academia\\PhD\\SEM 3 FALL 2022\\Deep Learning\\DLproject'\n",
    "#sys.path.append(path)\n",
    "#import final_proj_pack as fpp\n",
    "#from final_proj_pack import tune_arch, model_arch\n",
    "\n",
    "\n",
    "\n",
    "#create a Mymodel object\n",
    "cov_model = Mymodel(objective = 'pred_outcome', pathname = path)\n",
    "#proprocess the covid data set, this includes removing nuisance features and Nans\n",
    "#as well as one hot converting the categorical features\n",
    "cov_model.preprocess(convert_X = 'Onehot', drop_features_with_high_nan=True)\n",
    "#fit an initial architechture\n",
    "cov_model.build_model(conv_layers = None, \n",
    "                      mlp_layers = 5, \n",
    "                      mlp_hidden_act = ['relu']*5,\n",
    "                      hidden_units = [120, 60, 30, 20, 10], \n",
    "                      conv_hidden_act= ['relu'], \n",
    "                      padd_type = 'same', \n",
    "                      num_filters = [128], \n",
    "                      kernels = [5], \n",
    "                      add_norm = False, \n",
    "                      pool_type = 'Max',\n",
    "                      layers_stride = 3,\n",
    "                      use_metric = 'accuracy', \n",
    "                      use_batch_norm = True, \n",
    "                      use_dropout_layers = True, \n",
    "                      dropout_rate = 0.3,\n",
    "                      use_encoding_layer=False,\n",
    "                      out_mode='one_hot',\n",
    "                      verbose = True)\n",
    "\n",
    "#fit for one epoch to get the training and testing data split\n",
    "cov_model.fit(num_epochs = 1)\n",
    "\n",
    "#set the data split point\n",
    "split_point = round(cov_model.x_train.shape[0]*(1-0.2))\n",
    "\n",
    "#get the training and validation data\n",
    "x_train = cov_model.x_train[0:split_point,:]\n",
    "y_train = cov_model.y_train_convert[:split_point]\n",
    "x_valid = cov_model.x_train[split_point:cov_model.x_train.shape[0],:]\n",
    "y_valid = cov_model.y_train_convert[split_point:]\n",
    "\n",
    "#run keras tuner to get tune for number of units in 5 layer network\n",
    "tune_suggestion = tune_arch(x_train, x_valid, y_train, y_valid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_suggestion.build(input_shape = x_train.shape)\n",
    "tune_suggestion.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (278978, 100)             16700     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (278978, 100)            400       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (278978, 100)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (278978, 100)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (278978, 100)             10100     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (278978, 100)            400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (278978, 100)             0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (278978, 100)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (278978, 100)             10100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (278978, 100)            400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (278978, 100)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (278978, 100)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (278978, 100)             10100     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (278978, 100)            400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (278978, 100)             0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (278978, 100)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (278978, 100)             10100     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (278978, 100)            400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (278978, 100)             0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (278978, 100)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (278978, 1)               101       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (278978, 1)              4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (278978, 1)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (278978, 1)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,205\n",
      "Trainable params: 58,203\n",
      "Non-trainable params: 1,002\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tune_suggestion.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer0 (Dense)       (None, 120)               20040     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 120)               0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " hidden_layer1 (Dense)       (None, 100)               12100     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_layer2 (Dense)       (None, 80)                8080      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 80)                0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " hidden_layer3 (Dense)       (None, 50)                4050      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 50)               200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " hidden_layer4 (Dense)       (None, 20)                1020      \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 20)               80        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 20)                0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 21        \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 1)                4         \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,795\n",
      "Trainable params: 46,053\n",
      "Non-trainable params: 742\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1090/1090 [==============================] - 13s 9ms/step - loss: 0.4562 - accuracy: 0.8599 - val_loss: 0.3340 - val_accuracy: 0.8914\n",
      "Epoch 2/5\n",
      "1090/1090 [==============================] - 10s 9ms/step - loss: 0.3031 - accuracy: 0.8894 - val_loss: 0.2582 - val_accuracy: 0.8950\n",
      "Epoch 3/5\n",
      "1090/1090 [==============================] - 10s 9ms/step - loss: 0.2513 - accuracy: 0.8919 - val_loss: 0.2275 - val_accuracy: 0.8979\n",
      "Epoch 4/5\n",
      "1090/1090 [==============================] - 10s 9ms/step - loss: 0.2300 - accuracy: 0.8922 - val_loss: 0.2141 - val_accuracy: 0.8989\n",
      "Epoch 5/5\n",
      "1090/1090 [==============================] - 10s 9ms/step - loss: 0.2203 - accuracy: 0.8933 - val_loss: 0.2075 - val_accuracy: 0.8985\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.8994\n"
     ]
    }
   ],
   "source": [
    "#build final model according to tuner suggestions\n",
    "cov_model.build_model(conv_layers = None, \n",
    "                      mlp_layers = 5, \n",
    "                      mlp_hidden_act = ['relu']*5,\n",
    "                      hidden_units = [120, 100, 80, 50, 20], \n",
    "                      conv_hidden_act= ['relu'], \n",
    "                      padd_type = 'same', \n",
    "                      num_filters = [128], \n",
    "                      kernels = [5], \n",
    "                      add_norm = False, \n",
    "                      pool_type = 'Max',\n",
    "                      layers_stride = 3,\n",
    "                      use_metric = 'accuracy', \n",
    "                      use_batch_norm = True, \n",
    "                      use_dropout_layers = True, \n",
    "                      dropout_rate = 0.3,\n",
    "                      use_encoding_layer=False,\n",
    "                      out_mode='one_hot',\n",
    "                      verbose = True)\n",
    "\n",
    "\n",
    "#fit\n",
    "cov_model.fit(num_epochs = 5, batchsize = 256)\n",
    "#evaluate performance\n",
    "cov_model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
